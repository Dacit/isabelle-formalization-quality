\section{Analysis of Formalization Quality}\label{sec:quality}
In our previous work,
we did not find metrics for detecting defects in software systems suitable to detect lints in formalizations~\cite{Formal2022Huch}.
However, the concept of code smells in software is more closely related to formalization code,
because the concept of code anti-patterns is transferable to formalizations.
In fact,
the Isabelle linter offers detection for a wide range of different anti-patterns~\cite{Linter2022Megdiche},
but not all of them are universally agreed on to have a detrimental effect on quality---%
for instance, some are specifically designed for certain styles.
With the editors of the AFP, we discussed which of the available lints should be executed on new submissions.
A few checks were suitable only for new submissions (e.g., the use of phased-out Isabelle commands),
and the following subset was agreed on to indicate actual quality problems
(even for existing material)\footnote{
Some of those criteria are checked by a combination of multiple lints.}:
\begin{enumerate}
%\\%apply_isar_switch
\item \emph{Switch between apply-style and Isar}
as such proofs are \enquote{hacked together} and not written down properly
(and hence hard to read).
%\\%counter_example_finder,proof_finder
\item \emph{Counter-example and proof-finder commands}
which are left over from interactive use,
and only affect the proof-checking speed in the archive.
%,diagnostic_command
\item \emph{Diagnostic commands} which are left over as well,
though sometimes intentionally for documentation purposes.
%\\%global_attribute_changes
\item \emph{Changes of global lemma attributes}
as such changes should only be made in a local context
so as not to affect users of the formalization.
%\\%global_attribute_on_unnamed_lemma
\item \emph{Unnamed lemmas in prover collections}
(e.g., the simplification set),
as without a name to address them, those lemmas can't be disabled.
%\\%unrestricted_auto,simplifier_isar_initial_method
\item \emph{Use of auto-style methods as non-terminal proofs methods},
which makes the following proof steps brittle as they rely on goals generated by aggressive rewriting.
\item \emph{Structured proof starting with an auto-style method} which is an especially bad version of the former,
as then the entire proof structure can change.
%\\%complex_method,complex_isar_initial_method
\item \emph{Overly complex methods}
as they make it very hard to read and follow a proof.
%\\%tactic_proofs
\item \emph{Tactic proofs}
since they allow to refer to system-generated names
which are arbitrary and could change at any point,
so they should not be explicitly referenced.
\end{enumerate}
These lints have been adopted in the AFP submission process and appear to be useful,
but there is no quantifiable proof of their effectiveness yet.

To account for file size,
we define \emph{lint frequency} to be the number of lints reported per line of Isabelle source code.
Hence, lint frequency is independent of file size.
To classify theories as good or problematic,
we use a pre-defined threshold of one lint per $100$ lines.
While this threshold is a bit arbitrary,
it seems appropriate given our previous experience with lints in Isabelle and the AFP,
where we found that one lint occurred every $200$ lines in HOL and four times more often in other sessions~\cite{Linter2022Megdiche}
(albeit with a different selection of lints).

Change-sets and churn can easily be measured for the Isabelle and AFP repositories\footnote{Merge commits are omitted as they only exist for technical reasons.}.
Of course, not all changes are due to maintenance problems---%
many authors contribute further additions and improvements
(e.g., better naming) to their developments.
We consider change-sets affecting more than a \emph{single} development to represent maintenance effort,
assuming that developers generate coherent change-sets that don't put the codebase into an inoperable state---%
under that assumption,
change-sets affecting \emph{multiple} developments correspond to changes that break other entries which then need to be repaired,
which is maintenance effort by definition.
It should be noted that maintenance effort is not necessarily caused by poor maintainability of a theory:
some edits induced by upstream changes might be unavoidable,
e.g., re-naming of a lemma that is later explicitly mentioned.
Still, change frequency should be a reasonable measure to approximate maintainability.

We quantify the above metrics over the Isabelle and AFP libraries to answer the following specific research questions:
\begin{enumerate}[label=\bfseries{RQ\arabic*}:, leftmargin=*]
\item Which of the problems uncovered by lints actually increase maintenance effort?
\item Are formalizations where many lints occur structurally different from others?
\item How much do lints reflect on perceived quality? Do other models?
\end{enumerate}

Our data are based on the Isabelle2022 release,
excluding tools as they are mostly not written in Isabelle/Isar
(and hence not covered by the Linter),
example theories since they could differ a lot from regular material,
and particularly slow developments due to computation time considerations.
A total of $6470$ theory files remain.
In the sections that follow we address each research question individually.

\subsection{Maintenance Effort}
Since we rely on the notion of single or multiple AFP developments for change frequencies,
we do not consider theories of the Isabelle distribution itself here,
leaving us $6160$ AFP theories.
On average, such a theory file is changed every $506$ days, and $139$ lines are edited in a change.
Figure~\ref{fig:changefreq_violin} shows violin plots for the distribution of change frequencies for change-sets affecting a single and multiple developments,
separated by whether lints occur frequently (i.e., more often than one lint every $100$ lines) in the theory.
\input{figures/changefreq_violin}
The change frequency for maintenance change-sets is centered around zero,
whereas it is greater than zero for those affecting only a single development.
Also, while overall the distributions for frequent and infrequent lints are quite similar,
the peak at zero for maintenance change-sets is far more pronounced for theories with infrequent lints.

Both the number of lints in a theory file and its change frequency are dependent on the theory size 
(as measured in source lines of code)
since larger theories contain more code that can contain problems and can break.
Hence, we need to control for size when measuring the relationship between change frequency (or churn) and number of lints.
We do a partial analysis with Spearman correlation
(i.e., measuring how much one variable increases as the other increases, while accounting for the third).
Table~\ref{tab:lint_change_cor} shows the result.
\input{tables/lint_change_cor}
The observed relationships are not very strong,
but the results are significant at the $p<0.01$ level or below,
and most lints correlate positively with the frequency of maintenance changes.
Out of the three anti-patterns that would \emph{cause} maintenance incidents,
both auto as initial method and tactic proofs are positively correlated---%
tactic proofs have the strongest relationship with change frequency we observed with a value of $0.125$---%
but auto as non-terminal method has no significant impact.
It is noteworthy that during the discussion with the AFP editors,
Paulson, who built auto, objected that the pattern would be problematic,
arguing that the usage was as designed (though his vote was overruled).
When all change-sets are considered
(where maintenance and improvements are mixed),
there is almost no significant correlation;
for the change-sets concerning only single developments,
there are fewer significant correlations,
of which most are negative.
Again, tactic proofs are most strongly correlated with a value of $-0.136$.
One possible explanation is that authors replace tactics as part of their improvements---%
this is even explicitly mentioned four times in the AFP commit log.
Churn frequency is quite similar to change frequency overall,
and the correlation with tactic proofs is even a bit stronger with a value of $0.159$.
On the other hand, the correlation to non-terminal auto calls is negative,
which could be due to the fact that when such a proof breaks,
usually only a single line needs to be touched.
We also computed correlations for other lints not covered in our selection
(in the bottom half of the table),
and while the overall picture is the same,
the correlation with the warning about short names is negative for maintenance change-sets.
A possible explanation is that such identifiers are commonly used when logical foundations are formalized,
which is mostly done by experts who usually produce high-quality and robust formalizations.

\subsection{Lints and Formalization Structure}
To establish better metrics for formalization quality,
it is important to know how bad-quality formalizations are structurally different from good ones.
In our previous work,
we used graph metrics, such as number of neighbours, centrality scores, etc., computed over the Isabelle \emph{formal entity graph},
i.e., the dependency graph of logically relevant entities in a formalization.
Although graph metrics are used as quality indicators in software systems,
we found that these metrics are not good indicators of lint frequency for Isabelle code~\cite{Formal2022Huch}.

In this work, we investigate the feasibility of using these metrics with machine learning models
to differentiate between formalizations with frequent and infrequent lints.
In our investigation, we consider nearest neighbours, multilayer perceptron, decision tree, gradient-boosted tree, and random forest,
as representatives of classical models.
Feature inputs for these models are produced per theory and include size and aggregated formal entity graph properties, such as node degree and centrality.
Additionally,
we consider graph neural networks as representatives of deep learning models, motivated by successes for similar tasks~\cite{GNNDefect2022Sikic}.
We used graph convolutional networks, the GraphSAGE model~\cite{hamilton2018inductive}, and graph attention networks~\cite{veličković2018graph},
trained purely on the underlying formal entity graphs
(node degree as single feature).

We consider two scenarios: treating the task as binary classification
(i.e., using a binary label that stands for frequent or infrequent lints),
and as a regression task
(i.e., fitting the model directly to the lint frequency).
We use \SI{70}{\percent} of our data for training, \SI{15}{\percent} for validation
(hyper-parameter tuning and model selection)
and \SI{15}{\percent} final testing (to evaluate generalizability).
Hyper-parameters (i.e., additional parameters of the models) were tuned by random search on appropriate parameter grids,
which can be found in our published source code\footnote{\url{https://github.com/Dacit/isabelle-formalization-quality}}.
We evaluate all models in the classification setting by computing the average precision (AP) over all thresholds for each model.
To evaluate how well the final model performs for different thresholds,
we analyze the curve of precision (how many of the theories predicted as problematic were actually problematic) and recall (how many problematic theories were found).

Figure~\ref{fig:lint_freq_models_comp} shows the validation performance of our trained models.
\input{figures/lint_freq_models_comp}
It is evident that the deep learning models
do not perform better than classical models on our dataset,
which is not uncommon for data with noise in the ground truth
(of which we would expect quite a lot given the scenario).
Overall, the random forest model trained as a classifier is the best model with a validation AP of $0.47$.
Its precision-recall curve on the test data is shown in Figure~\ref{fig:rf_prc}.
\input{figures/rf_prc}
With a test AP of $0.51$, the model generalizes quite well---%
for example, \SI{58}{\percent} precision and \SI{21}{\percent} recall can be reached at a default threshold of $0.5$,
and the model is far better than random on the whole range.
This shows that while lints cannot be predicted very accurately,
they do reflect strongly on the formal entity graphs.

To find out which features are most important,
we perform an ablation study with the random forest model,
i.e., we iteratively re-train the model, removing the feature that leads to the smallest decrease in validation AP\@.
Figure~\ref{fig:lint_freq_rf_feats} shows the result.
\input{figures/lint_freq_rf_feats}
Initially, the score fluctuates slightly,
until it eventually takes a steep decline---%
at the turning point,
betweenness centrality (median and max), out-degree (max), closeness centrality (mean), and in-degree (mean),
remain.

\subsection{Comparison with Perceived Quality}\label{sec:llama}
Obtaining a dataset for perceived quality is quite difficult,
since often only domain experts are capable of judging the quality of a given formalization,
especially when it comes to quality problems that are not obvious.
By inquiring Isabelle experts in various domains about theories or AFP developments they would consider good and bad quality,
we obtained a ground-truth sample of $70$ labelled theories
($35$ good and bad each).
Importantly,
we did not give a definition of quality
(relying on the experts' intuition)
nor did we contribute any data ourselves,
so as not to skew the dataset in any way.
As an example,
one expert classified a theory as problematic because the assumptions on the main theorem were stronger than needed,
making it unusable to them.

While $70$ examples is not enough data to train any models on,
the sample is sufficiently large to evaluate both the lint results and our other machine learning model discussed previously.
In addition,
we evaluate the un-tuned capabilities of a large language model:
By prompting the pre-trained LLAMA ($7$ billion parameters) transformer~\cite{touvron2023llama}
about the quality of chunks of formalization code
(on a scale from $1$ to $10$)
and averaging the scores for each theory,
we obtained quality predictions for the whole AFP\@.
Even though we did not fine-tune the model due to the prohibitive hardware requirements,
its responses sound mostly coherent, e.g., a score of $2$ was given with the justification that \enquote{the lemma \texttt{equivclp\_least[OF hash]} is a bit cryptic and I would prefer something more explicit and readable}.

With our pre-defined threshold of one lint every $100$ lines,
we achieved \SI{95}{\percent} precision and \SI{51}{\percent} recall
at predicting bad-quality theories on our sample,
whereas the random forest model scored \SI{43}{\percent} precision and \SI{100}{\percent} recall.
\input{figures/ground_prcs}
Figure~\ref{fig:ground_prcs} shows the full precision-recall curves for our three classifiers.
The lint frequency remains an excellent estimator even for high thresholds where recall goes up to \SI{77}{\percent} (at \SI{79}{\percent} precision),
and is outperformed by the random forest model only in the low-recall regions
(where the latter attains perfect precision for up to \SI{43}{\percent} recall).
The large language model performs well in the low-recall regions,
which is surprising given that it has not been trained on Isabelle quality at all,
suggesting that textual structure alone
(which is what the model understands)
can give a hint about overall quality.
However, for higher thresholds,
it is close to random.
